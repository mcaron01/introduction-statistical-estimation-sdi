{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d023c0",
   "metadata": {},
   "source": [
    "# TP2 - MCMC\n",
    "\n",
    "## Estimation avancée - G3 SDIA\n",
    "\n",
    "Dans ce TP, on s'intéresse aux méthodes d'échantillonnage dites \"MCMC\" (Monte Carlo par Chaînes de Markov). Le premier exercice consiste à implémenter un Metropolis-Hastings et de regarder l'influence de quelques paramètres (disponible dans le premier notebook). Le deuxième exercice met en oeuvre une méthode de régression bayésienne linéaire.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Renommer votre notebook sous la forme `tp2b_Nom1_Nom2.ipynb`, et inclure le nom du binôme dans le notebook. \n",
    "\n",
    "2. Votre code, ainsi que toute sortie du code, doivent être commentés !\n",
    "\n",
    "3. Déposer votre notebook sur Moodle dans la section prévue à cet effet avant la date limite : 10 Décembre 2023, 23h59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8451fed",
   "metadata": {},
   "source": [
    "### Partie 2 - Régression linéaire bayésienne parcimonieuse\n",
    "\n",
    "On suppose le modèle de régression linéaire suivant :\n",
    "$$ y_i \\sim \\mathcal{N}(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2)$$\n",
    "\n",
    "On a :\n",
    "* $y \\in \\mathbb{R}$, la variable que l'on cherche à prédire ;\n",
    "* $\\mathbf{x}_i \\in \\mathbb{R}^p$, les features ;\n",
    "* $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$, le vecteur de régression ;\n",
    "* $\\sigma^2$ la variance du bruit.\n",
    "\n",
    "Nous adoptons une approche bayésienne. Les paramètres du modèle, $\\boldsymbol{\\beta}$ et $\\sigma^2$, doivent donc être munis d'une loi a priori.\n",
    "\n",
    "Dans cet exercice, nous allons suivre le modèle du *Bayesian LASSO* ([Park et al. (2008)](https://people.eecs.berkeley.edu/~jordan/courses/260-spring09/other-readings/park-casella.pdf)).\n",
    "\n",
    "Le LASSO est une méthode de régression pénalisée classique (c'est la pénalisation en norme $\\ell_1$ - vous connaissez la pénalisation en norme $\\ell_2$ appelée *ridge regression*). Nous regardons ici sa version bayésienne. Les lois a priori sont les suivantes :\n",
    "\\begin{align}\n",
    "\\sigma^2 & \\sim \\text{IG}(a_0, b_0) \\\\\n",
    "\\beta_p | \\sigma^2 & \\sim \\text{Laplace} \\left( 0, \\frac{\\sqrt{\\sigma^2}}{\\lambda} \\right)\n",
    "\\end{align}\n",
    "\n",
    "$\\lambda$ est le paramètre de régularisation. Il se trouve qu'en prenant une loi a priori de [Laplace](https://en.wikipedia.org/wiki/Laplace_distribution) pour les coefficients de régression, le problème du LASSO est le même que celui l'estimation MAP.\n",
    "\n",
    "À partir des données $\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1), ... (\\mathbf{x}_n, y_n) \\}$, l'objectif est donc de caractériser la loi a posteriori $p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathcal{D})$. Elle n'est pas tractable analytiquement. Nous allons donc échantillonner de cette loi à l'aide d'un algorithme MCMC, plus précisémment, un [échantillonneur de Gibbs](https://fr.wikipedia.org/wiki/%C3%89chantillonnage_de_Gibbs).\n",
    "\n",
    "Il n'est pas possible de dériver un échantillonneur de Gibbs immédiatement. Il se trouve que le modèle peut être ré-écrit de la manière suivante :\n",
    "\\begin{align}\n",
    "\\sigma^2 & \\sim \\text{IG}(a_0, b_0) \\\\\n",
    "\\tau_i^2 & \\sim \\text{Exp} \\left( \\frac{\\lambda^2}{2} \\right) \\\\\n",
    "\\beta_j | \\tau_j^2, \\sigma^2 & \\sim \\mathcal{N}(0, \\sigma^2 \\tau_j^2) \\\\\n",
    "y_i | \\boldsymbol{\\beta}, \\sigma^2 & \\sim \\mathcal{N}(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "On peut alors trouver les lois conditionnelles :\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\beta} | \\boldsymbol{\\tau}, \\sigma^2, \\mathcal{D} \\sim \\mathcal{N}(\\mathbf{A}^{-1} \\mathbf{X}^{\\top} \\mathbf{y}, \\sigma^2 \\mathbf{A}),\n",
    "\\end{align}\n",
    "avec $\\mathbf{A} = (\\mathbf{X}^{\\top} \\mathbf{X} + \\mathbf{D}_{\\tau})^{-1}$, où $\\mathbf{D}_{\\tau} = \\text{diag}(\\tau_1^{-2}, ..., \\tau_p^{-2})$.\n",
    "\\begin{align}\n",
    "\\tau_j^{-2} | \\boldsymbol{\\beta}, \\sigma^2, \\mathcal{D} \\sim \\text{InvGaussian}\\left( \\sqrt{\\frac{\\lambda^2 \\sigma^2}{\\beta_j^2}}, \\lambda^2 \\right),\n",
    "\\end{align}\n",
    "(Loi \"Inverse-Gaussian\" -> [ici](https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution))\n",
    "\\begin{align}\n",
    "\\sigma^2 | \\boldsymbol{\\beta}, \\boldsymbol{\\tau}, \\mathcal{D} \\sim \\text{IG}\\left( a_0 + \\frac{n+p}{2}, b_0 + \\frac{1}{2} || \\mathbf{y - X} \\boldsymbol{\\beta} ||^2_2 + \\frac{1}{2} \\sum_{j=1}^p \\frac{\\beta_j^2}{\\tau_j^2} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the data\n",
    "\n",
    "# Load the dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Print dataset description\n",
    "print(diabetes.DESCR)\n",
    "\n",
    "# Features\n",
    "X = diabetes.data\n",
    "X = X/np.std(X, axis = 0) # Standardize\n",
    "N,P = X.shape\n",
    "\n",
    "# Target\n",
    "y = diabetes.target\n",
    "y = (y-np.mean(y))/np.std(y) # Standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ad6c2",
   "metadata": {},
   "source": [
    "**Q1.** Écrire une fonction implémentant l'échantillonnage de Gibbs dans ce modèle, qui prend en arguments :\n",
    "* Les données sous forme matricielle $\\mathbf{X}$ et $\\mathbf{y}$\n",
    "* Le nombre d'échantillons $N_g$\n",
    "* La taille du *burn-in* $N_b$\n",
    "* Les hyper-paramètres du modèle : $a_0, b_0, \\lambda$\n",
    "\n",
    "Elle retourne $N_g$ échantillons du posterior $p(\\boldsymbol{\\beta}, \\sigma^2 | \\mathcal{D})$.\n",
    "\n",
    "**Attention !** Bien lire la doc. de `scipy.stats.invgauss` pour savoir quels paramètres utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_lasso_gibbs(X, y, Ng, Nb, a0, b0, l):\n",
    "    #######\n",
    "    ## YOUR CODE HERE\n",
    "    #######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daab870",
   "metadata": {},
   "source": [
    "**Q2.** Faire tourner la chaîne MCMC avec les paramètres suivants : $N_g = N_b = 1000$, $a_0 = b_0 = 1$, $\\lambda = 10$.\n",
    "\n",
    "* Afficher le *traceplot* pour un paramètre de votre choix. Commenter.\n",
    "* Pour chacun des paramètres $\\boldsymbol{\\beta}$ et $\\sigma^2$, afficher un histogramme ou un KDE des $N_g$ échantillons obtenus. Afficher sur le même plot la valeur de l'estimation MMSE et MAP. Enfin, donner l'intervalle de crédibilité à 95%. (Cf. dernier chapitre du cours Intro à l'Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ng = 1000\n",
    "Nb = 1000\n",
    "a0 = 1\n",
    "b0 = 1\n",
    "l = 10\n",
    "\n",
    "#######\n",
    "## YOUR CODE HERE\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057aa92",
   "metadata": {},
   "source": [
    "**Q3.** Générer 1000 échantillons de la loi prédictive a posteriori pour le nouveau point $x_{\\text{new}}$ (défini dans le code) et tracer un histogramme ou un KDE. Commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([ 1.41145807,  1.06548848,  0.30006161,  0.45984057, -0.52475728,\n",
    "       -1.70643289,  1.02259953,  1.49710409, -1.25030999,  0.84817082])\n",
    "\n",
    "#######\n",
    "## YOUR CODE HERE\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c7d53",
   "metadata": {},
   "source": [
    "**Q4.** Étudier l'influence du paramètre lambda sur l'inférence des paramètres.\n",
    "\n",
    "Sans l'implémenter, à l'aide de vos connaissances, pouvez-vous proposer une méthode permettant de choisir lambda ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446afaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## YOUR CODE HERE\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f439c",
   "metadata": {},
   "source": [
    "**Questions bonus**. Le TP est déjà long. Votre objectif premier est d'effectuer les questions précédentes correctement.\n",
    "\n",
    "S'il vous reste du temps et de la motivation, voici deux questions \"papier-crayon\" :\n",
    "* Montrer que l'estimation MAP dans un modèle avec un *prior* Laplace pour les coefficients de régression mène bien au même problème que le LASSO\n",
    "* Dériver la loi conditionnelle pour $\\boldsymbol{\\beta}$ ou $\\sigma^2$. Attention ! Le calcul de la loi conditionnelle de $\\tau_i$ est quant à lui très compliqué. Ne le tentez pas sans l'aide d'un(e) professionnel(le)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
