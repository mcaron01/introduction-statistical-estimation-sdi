{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1296a3d",
   "metadata": {},
   "source": [
    "# TP3 - *Latent Dirichlet Allocation* et Inférence variationnelle \n",
    "\n",
    "## Estimation avancée - G3 SDIA\n",
    "\n",
    "Dans ce TP, on s'intéresse à la méthode \"inférence variationnelle\" (VI) qui permet d'approcher la loi a posteriori d'un modèle (généralement inconnue) par une autre loi plus simple (généralement un produit de lois bien connues). Nous allons l'appliquer à un modèle probabiliste pour des données textuelles, appelé *Latent Dirichlet Allocation* (LDA, qui n'a rien à voir avec la LDA *Linear Discriminant Analysis* du cours de ML).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Renommer votre notebook sous la forme `tp3_Nom1_Nom2.ipynb`, et inclure le nom du binôme dans le notebook. \n",
    "\n",
    "2. Votre code, ainsi que toute sortie du code, doivent être commentés !\n",
    "\n",
    "3. Déposer votre notebook sur Moodle dans la section prévue à cet effet avant la date limite : 23 Décembre 2023, 23h59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d382bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22423210",
   "metadata": {},
   "source": [
    "### Partie 0 - Introduction\n",
    "\n",
    "LDA is a popular probabilistic model for text data, introducted in [Blei et al. (2003)](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). In this model, the posterior distribution is intractable, and we choose to resort to variational inference (note that a Gibbs sampler would be feasible as well, but would be very slow). In particular, the CAVI updates can be easily derived.\n",
    "\n",
    "In a few words, in LDA, each document is a mixture of topics, and each topic is a mixture of words. Uncovering those is the goal of *topic modeling*, and this is what we are going to do today. We will be using a collection of abstracts of papers published in JMLR (*Journal of Machine Learning Research*), one of the most prominent journals of the field.\n",
    "\n",
    "**Check the .pdf file describing the model.**\n",
    "The posterior is :\n",
    "$$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\mathbf{z} | \\mathcal{D}),$$\n",
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059097f",
   "metadata": {},
   "source": [
    "### Partie 1 - Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf0e5b",
   "metadata": {},
   "source": [
    "The data is already prepared, see code below. We have a total of 1898 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41348f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmlr_papers = pkl.load(open(\"jmlr.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694906",
   "metadata": {},
   "source": [
    "**Q1.** Fill in a list of keywords from the course, to see how many papers are about probabilistic ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321a5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 501 Bayesian papers out of 1898\n"
     ]
    }
   ],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = ['bayesian', 'probabilistic', 'Markov', 'mcmc', 'Gibbs sampling', 'posterior', 'prior', \n",
    "                         'likelihood', 'Monte Carlo', 'variational inference']\n",
    "    if any([kwd in paper[\"abstract\"] for kwd in bayesian_keywords]):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "        \n",
    "print(\"There are\", str(len(bayesian_jmlr_papers))+\" Bayesian papers out of\", str(len(jmlr_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a047cf9",
   "metadata": {},
   "source": [
    "Let us now preprocess the data. It is important to remove so-called \"stop-words\" like a, is, but, the, of, have... Scikit-learn will do the job for us. We will keep only the top-1000 words from the abstracts.\n",
    "\n",
    "As a result, we get the count matrix $\\mathbf{C}$ of size $D = 1898 \\times V = 1000$. $c_{dv}$ is the number of occurrences of word $v$ in document $d$. This compact representation is called \"bag-of-words\". Of course from $\\mathbf{C}$ you easily recover the words, since in LDA the order does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855d2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '16', '17', '18', '949', '_blank', 'ability', 'able', 'abs', 'according', 'account', 'accuracy', 'accurate', 'achieve', 'achieved', 'achieves', 'action', 'actions', 'active', 'adaboost', 'adaptive', 'addition', 'additional', 'additive', 'address', 'advantage', 'advantages', 'agent', 'aggregation', 'al', 'algorithm', 'algorithmic', 'algorithms', 'allow', 'allowing', 'allows', 'alternative', 'analysis', 'analyze', 'applicable', 'application', 'applications', 'applied', 'apply', 'applying', 'approach', 'approaches', 'appropriate', 'approximate', 'approximately', 'approximation', 'approximations', 'arbitrary', 'art', 'article', 'artificial', 'associated', 'assume', 'assumed', 'assumption', 'assumptions', 'asymptotic', 'asymptotically', 'attributes', 'available', 'average', 'averaging', 'bandit', 'base', 'based', 'basic', 'basis', 'batch', 'bayes', 'bayesian', 'behavior', 'belief', 'benchmark', 'best', 'better', 'bias', 'bib', 'binary', 'block', 'boosting', 'bound', 'bounded', 'bounds', 'br', 'build', 'building', 'called', 'capture', 'carlo', 'case', 'cases', 'causal', 'central', 'certain', 'chain', 'challenge', 'challenging', 'characterization', 'characterize', 'choice', 'chosen', 'class', 'classes', 'classical', 'classification', 'classifier', 'classifiers', 'close', 'closed', 'cluster', 'clustering', 'clusters', 'code', 'coding', 'coefficient', 'coefficients', 'collection', 'color', 'com', 'combination', 'combine', 'combined', 'combining', 'common', 'commonly', 'community', 'comparable', 'compare', 'compared', 'comparing', 'comparison', 'comparisons', 'competitive', 'complete', 'complex', 'complexity', 'component', 'components', 'compression', 'computation', 'computational', 'computationally', 'compute', 'computed', 'computer', 'computing', 'concept', 'concepts', 'condition', 'conditional', 'conditions', 'confidence', 'consider', 'considered', 'consistency', 'consistent', 'consists', 'constant', 'constrained', 'constraint', 'constraints', 'construct', 'constructing', 'construction', 'context', 'continuous', 'contrast', 'contribution', 'control', 'converge', 'convergence', 'convex', 'core', 'correct', 'correlation', 'corresponding', 'cost', 'costs', 'covariance', 'criteria', 'criterion', 'cross', 'current', 'curve', 'dag', 'data', 'datasets', 'deal', 'decision', 'decomposition', 'deep', 'define', 'defined', 'definite', 'degree', 'demonstrate', 'demonstrated', 'density', 'dependence', 'dependencies', 'dependent', 'depends', 'derive', 'derived', 'descent', 'described', 'describes', 'design', 'designed', 'detection', 'determine', 'deterministic', 'develop', 'developed', 'dictionary', 'difference', 'differences', 'different', 'difficult', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'direct', 'directed', 'direction', 'directly', 'dirichlet', 'discovery', 'discrete', 'discriminant', 'discriminative', 'discuss', 'discussed', 'distance', 'distances', 'distributed', 'distribution', 'distributions', 'divergence', 'document', 'does', 'domain', 'domains', 'dual', 'dynamic', 'dynamics', 'easily', 'easy', 'edges', 'effect', 'effective', 'effectiveness', 'effects', 'efficiency', 'efficient', 'efficiently', 'elements', 'ell_1', 'em', 'embedding', 'empirical', 'empirically', 'enables', 'end', 'ensemble', 'entries', 'entropy', 'environment', 'epsilon', 'equivalence', 'equivalent', 'error', 'errors', 'especially', 'establish', 'established', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'estimator', 'estimators', 'et', 'euclidean', 'evaluate', 'evaluated', 'evaluation', 'evidence', 'exact', 'example', 'examples', 'exist', 'existence', 'existing', 'exists', 'expectation', 'expected', 'experimental', 'experiments', 'expert', 'experts', 'explicit', 'explicitly', 'exploit', 'exploiting', 'exploration', 'explore', 'exponential', 'expression', 'extend', 'extended', 'extension', 'extensions', 'extensive', 'fact', 'factor', 'factorization', 'factors', 'families', 'family', 'fast', 'faster', 'feature', 'features', 'field', 'fields', 'filtering', 'finally', 'finding', 'finite', 'fisher', 'fit', 'fitting', 'fixed', 'flexible', 'focus', 'following', 'font', 'form', 'formulation', 'formulations', 'framework', 'free', 'fully', 'function', 'functional', 'functions', 'fundamental', 'furthermore', 'future', 'games', 'gap', 'gaussian', 'gene', 'general', 'generalization', 'generalize', 'generalized', 'generally', 'generated', 'generating', 'generative', 'generic', 'geometric', 'geometry', 'github', 'given', 'gives', 'global', 'goal', 'good', 'gp', 'gradient', 'graph', 'graphical', 'graphs', 'gray', 'greedy', 'group', 'groups', 'guarantee', 'guarantees', 'hand', 'handle', 'hard', 'help', 'hidden', 'hierarchical', 'high', 'higher', 'highly', 'hilbert', 'hold', 'href', 'http', 'human', 'hypotheses', 'hypothesis', 'ica', 'idea', 'identify', 'identifying', 'ii', 'illustrate', 'image', 'images', 'implement', 'implementation', 'implementations', 'implemented', 'importance', 'important', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'include', 'includes', 'including', 'increasing', 'independence', 'independent', 'index', 'indicate', 'individual', 'induced', 'inference', 'infinite', 'influence', 'information', 'input', 'inputs', 'instance', 'instances', 'instead', 'interactions', 'interesting', 'interpretation', 'inthe', 'intractable', 'introduce', 'introduced', 'introduces', 'invariant', 'inverse', 'investigate', 'involves', 'involving', 'issue', 'issues', 'items', 'iteration', 'iterations', 'iterative', 'joint', 'kernel', 'kernels', 'key', 'knowledge', 'known', 'label', 'labeled', 'labels', 'lambda', 'language', 'large', 'larger', 'lasso', 'latent', 'layer', 'lead', 'leading', 'leads', 'learn', 'learned', 'learner', 'learners', 'learning', 'learns', 'level', 'li', 'library', 'like', 'likelihood', 'limit', 'limited', 'line', 'linear', 'linearly', 'literature', 'local', 'locally', 'log', 'logarithmic', 'logistic', 'long', 'loss', 'losses', 'low', 'lower', 'machine', 'machines', 'magnitude', 'main', 'make', 'makes', 'making', 'manifold', 'manner', 'map', 'margin', 'marginal', 'markov', 'match', 'matching', 'mathbb', 'mathcal', 'matlab', 'matrices', 'matrix', 'max', 'maximization', 'maximum', 'mcmc', 'mean', 'means', 'measure', 'measurements', 'measures', 'memory', 'message', 'method', 'methodology', 'methods', 'metric', 'minimal', 'minimax', 'minimization', 'minimize', 'minimizing', 'minimum', 'mining', 'missing', 'mixed', 'mixture', 'mixtures', 'model', 'modeling', 'models', 'monte', 'motivated', 'multi', 'multiclass', 'multiple', 'multivariate', 'mutual', 'naive', 'natural', 'naturally', 'nature', 'nbsp', 'near', 'nearest', 'necessary', 'need', 'needed', 'negative', 'neighbor', 'network', 'networks', 'neural', 'new', 'node', 'nodes', 'noise', 'noisy', 'non', 'nonlinear', 'nonparametric', 'norm', 'normal', 'norms', 'notion', 'novel', 'np', 'number', 'numbers', 'numerical', 'object', 'objective', 'objects', 'observation', 'observations', 'observed', 'obtain', 'obtained', 'obtaining', 'offers', 'ofthe', 'ones', 'online', 'onthe', 'open', 'operator', 'optimal', 'optimality', 'optimization', 'oracle', 'order', 'original', 'outperform', 'outperforms', 'output', 'outputs', 'overall', 'pac', 'package', 'pair', 'pairs', 'pairwise', 'paper', 'papers', 'parallel', 'parameter', 'parameters', 'parametric', 'partial', 'particular', 'particularly', 'partition', 'path', 'pattern', 'patterns', 'pca', 'pdf', 'penalized', 'penalty', 'perform', 'performance', 'performed', 'performs', 'perspective', 'phase', 'point', 'points', 'policies', 'policy', 'polynomial', 'popular', 'population', 'positive', 'possible', 'posterior', 'potential', 'power', 'powerful', 'practical', 'practice', 'precision', 'predict', 'predicting', 'prediction', 'predictions', 'predictive', 'predictor', 'predictors', 'presence', 'present', 'presented', 'presents', 'previous', 'previously', 'principal', 'principle', 'prior', 'priors', 'privacy', 'probabilistic', 'probabilities', 'probability', 'problem', 'problems', 'procedure', 'procedures', 'process', 'processes', 'processing', 'produce', 'product', 'program', 'programming', 'projection', 'propagation', 'properties', 'property', 'propose', 'proposed', 'provably', 'prove', 'provide', 'provided', 'provides', 'providing', 'purpose', 'python', 'quadratic', 'quality', 'queries', 'question', 'random', 'randomized', 'range', 'rank', 'ranking', 'rate', 'rates', 'real', 'recent', 'recently', 'recognition', 'recovery', 'reduce', 'reduced', 'reduces', 'reduction', 'regression', 'regret', 'regularization', 'regularized', 'reinforcement', 'related', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relaxation', 'relevant', 'relies', 'rely', 'represent', 'representation', 'representations', 'represented', 'reproducing', 'require', 'required', 'requires', 'research', 'respect', 'response', 'restricted', 'result', 'resulting', 'results', 'reward', 'right', 'risk', 'rkhs', 'robust', 'robustness', 'role', 'rule', 'rules', 'run', 'running', 'sample', 'sampled', 'samples', 'sampling', 'scalable', 'scale', 'scales', 'scenarios', 'scheme', 'schemes', 'score', 'scoring', 'search', 'second', 'select', 'selection', 'semi', 'sense', 'sensitive', 'separation', 'sequence', 'sequences', 'sequential', 'series', 'set', 'sets', 'setting', 'settings', 'sgd', 'short', 'showing', 'shown', 'shows', 'showthat', 'signal', 'signals', 'significant', 'significantly', 'similar', 'similarity', 'simple', 'simulated', 'simulation', 'simulations', 'simultaneously', 'single', 'situations', 'size', 'sizes', 'small', 'smaller', 'smooth', 'social', 'soft', 'software', 'solution', 'solutions', 'solve', 'solved', 'solving', 'source', 'sources', 'space', 'spaces', 'sparse', 'sparsity', 'special', 'specific', 'specifically', 'specified', 'spectral', 'speed', 'squared', 'squares', 'stability', 'stable', 'stage', 'standard', 'state', 'stationary', 'statistical', 'statistics', 'step', 'steps', 'stochastic', 'strategies', 'strategy', 'strong', 'strongly', 'structural', 'structure', 'structured', 'structures', 'studied', 'studies', 'study', 'sub', 'subset', 'subsets', 'subspace', 'success', 'successfully', 'sufficient', 'suggest', 'suitable', 'sum', 'sup', 'supervised', 'support', 'surrogate', 'svm', 'svms', 'symmetric', 'synthetic', 'systems', 'table', 'takes', 'target', 'task', 'tasks', 'tdalign', 'technique', 'techniques', 'temporal', 'tensor', 'term', 'terms', 'test', 'testing', 'tests', 'text', 'thatthe', 'theorem', 'theoretic', 'theoretical', 'theoretically', 'theory', 'thispaper', 'threshold', 'tight', 'time', 'times', 'tool', 'toolbox', 'tools', 'topic', 'total', 'tr', 'tractable', 'trade', 'traditional', 'trained', 'training', 'transfer', 'treatment', 'tree', 'trees', 'true', 'type', 'types', 'typically', 'uncertainty', 'underlying', 'understanding', 'unified', 'uniform', 'universal', 'unknown', 'unlabeled', 'unlike', 'unsupervised', 'update', 'updates', 'upper', 'use', 'used', 'useful', 'usefulness', 'user', 'users', 'uses', 'using', 'usually', 'utility', 'v19', 'validation', 'value', 'valued', 'values', 'variable', 'variables', 'variance', 'variant', 'variants', 'variational', 'variety', 'various', 'varying', 'vector', 'vectors', 'version', 'versions', 'view', 'viewed', 'volume19', 'walk', 'way', 'weak', 'web', 'weight', 'weighted', 'weights', 'weshow', 'wide', 'widely', 'width', 'word', 'words', 'work', 'works', 'world', 'worst', 'years', 'yields', 'zero']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, stop_words='english')\n",
    "X = vectorizer.fit_transform([paper[\"abstract\"] for paper in jmlr_papers])\n",
    "print(vectorizer.get_feature_names()) # Top-1000 words\n",
    "C = X.toarray() # Count matrix\n",
    "\n",
    "# Removing documents with 0 words\n",
    "idx = np.where(np.sum(C, axis = 1)==0)\n",
    "C = np.delete(C, idx, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e3267",
   "metadata": {},
   "source": [
    "**Q2.** How many elements of $\\mathbf{C}$ are non-zero ? Is this surprising ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2d37bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total d'entrées dans la matrice C est : 1895000\n",
      "Le nombre d'élément non nul dans la matrice C est : 85804\n",
      "La proportion d'éléments non nulle dans cette matrice est donc : 4.528%\n"
     ]
    }
   ],
   "source": [
    "number_total_entries = len(C) * len(C[0])\n",
    "print(\"Le nombre total d'entrées dans la matrice C est :\", number_total_entries)\n",
    "\n",
    "num_nonzero_elements = np.count_nonzero(C)\n",
    "print(\"Le nombre d'élément non nul dans la matrice C est :\", num_nonzero_elements)\n",
    "\n",
    "print(f\"La proportion d'éléments non nulle dans cette matrice est donc : {num_nonzero_elements / number_total_entries * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e509bb",
   "metadata": {},
   "source": [
    "Ces résultats montrent que sur les 1895 documents et 1000 mots possibles, seuls 85 804 de ces éléments sont différents de zéro. La représentation du \"bag-of-words\" est donc creuse pour des ensembles de données textuelles (on a une matrice avec beaucoup de 0)\n",
    "\n",
    "Ceci n'est pas surprenant car chaque document utilise généralement seulement un petit sous-ensemble de l'ensemble du vocabulaire, conduisant à une matrice où la majorité des entrées sont nulles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72726d44",
   "metadata": {},
   "source": [
    "### Partie 2 - Inférence variationnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada0b7f",
   "metadata": {},
   "source": [
    "As you know from the lecture, VI aims at maximizing the ELBO. I have prepared for you the function to compute the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def ELBO(L, G, phi, a, e, W):\n",
    "    # Computes the ELBO with the values of the parameters L (Lambda), G (Gamma), and Phi\n",
    "    # a, e are hyperparameters (alpha and eta)\n",
    "    # W are the words (obsereved)\n",
    "    \n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    # a - Scalar > 0 (hyperparameter alpha)\n",
    "    # e - Scalar > 0 (hyperparameter eta)\n",
    "    # W - List of D elements, each element is a Nd x V matrix (observed words)\n",
    "    \n",
    "    e_log_B = (digamma(L).T - digamma(np.sum(L, axis = 1))).T\n",
    "    e_log_T = (digamma(G).T - digamma(np.sum(G, axis = 1))).T\n",
    "    \n",
    "    t1 = (e-1)*np.sum(e_log_B)\n",
    "    t2 = (a-1)*np.sum(e_log_T)\n",
    "\n",
    "    phi_s = np.zeros((D,K))\n",
    "    for d in range(0,D):\n",
    "        phi_s[d,:] = np.sum(phi[d], axis = 0)\n",
    "    t3 = np.sum(e_log_T*phi_s)\n",
    "    \n",
    "    tmp = np.zeros((K,V))\n",
    "    for d in range(0,D):\n",
    "        tmp = tmp + np.dot(phi[d].T, W[d])\n",
    "    t4 = np.sum(e_log_B*tmp)\n",
    "    \n",
    "    t5 = np.sum(loggamma(np.sum(L, axis = 1))) - np.sum(loggamma(L)) + np.sum((L-1)*e_log_B)\n",
    "    t6 = np.sum(loggamma(np.sum(G, axis = 1))) - np.sum(loggamma(G)) + np.sum((G-1)*e_log_T)\n",
    "\n",
    "    t7 = 0\n",
    "    for d in range(0,D):\n",
    "        t7 = t7 + np.sum(phi[d]*np.log(phi[d] + np.spacing(1)))\n",
    "\n",
    "    return t1 + t2 + t3 + t4 - t5 - t6 - t7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc0302",
   "metadata": {},
   "source": [
    "**Q1.** Transform the matrix $\\mathbf{C}$ into the observed words $\\mathbf{w}$. $\\mathbf{w}$ should be a list of $D$ elements, each element of the list being a $N_d \\times V$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2629d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, V = C.shape\n",
    "\n",
    "# Liste pour stocker les mots observés\n",
    "w = []\n",
    "\n",
    "for doc in range(D):\n",
    "    # On extrait la ligne des counts\n",
    "    counts_d = C[doc, :]\n",
    "    n_d = sum(counts_d)\n",
    "    \n",
    "    # Create a matrix with counts for each word in the vocabulary\n",
    "    matrix_d = np.zeros((n_d, V))\n",
    "\n",
    "    i = 0\n",
    "    k = 0\n",
    "    for v in range(len(counts_d)):\n",
    "        if counts_d[v] != 0:\n",
    "            for j in range(counts_d[v]):\n",
    "                matrix_d[i, k] = 1\n",
    "                i += 1\n",
    "            k += 1\n",
    "    \n",
    "    w.append(matrix_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0aa3d",
   "metadata": {},
   "source": [
    "**Q2.** Implement the CAVI algorithm. The updates are given at the beginning of the notebook. Monitor the convergence with the values of the ELBO (but start with a fixed number of iterations, like 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b24024",
   "metadata": {},
   "source": [
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19389deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI(W, K, a, e, seed, max_iters=50, tol=1e-5): # Other arguments may be added\n",
    "    \"\"\"\n",
    "    Coordinate Ascent Variational Inference (CAVI) for Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "    Parameters:\n",
    "    - W (list): List of D elements, each element is a Nd x V matrix (observed words).\n",
    "    - K (int): Number of topics.\n",
    "    - a (float): Hyperparameter for Gamma distribution (document-topic distribution).\n",
    "    - e (float): Hyperparameter for Gamma distribution (word-topic distribution).\n",
    "    - seed (int): Seed for random number generation.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - tol (float): Convergence tolerance.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Nombre de documents\n",
    "    D = len(W)\n",
    "    # Taille du vocabulaire\n",
    "    V = W[0].shape[1]\n",
    "\n",
    "    # Initialisation des paramètres variationnels\n",
    "    L = np.random.rand(K, V)\n",
    "    G = np.random.rand(D, K)\n",
    "    phi = [np.zeros((Wd.shape[0], K)) for Wd in W]\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Mise à jour phi\n",
    "        for d in range(D):\n",
    "            for n in range(len(phi[d])):\n",
    "                # Indice où il y a le 1 dans la matrice W[d][n, :]\n",
    "                w_dn = np.where(W[d][n, :] == 1)[0][0]\n",
    "                # Calcul de log_phi_dn pour tous les thèmes en une seule opération\n",
    "                phi[d][n, :] = np.exp(digamma(G[d, :]) + digamma(L[:, w_dn]) - digamma(np.sum(L, axis=1)))\n",
    "                # Normalisation\n",
    "                phi[d][n, :] /= np.sum(phi[d][n, :])\n",
    "\n",
    "        # Calcul de l'ELBO et vérification de la convergence\n",
    "        current_ELBO = ELBO(L, G, phi, a, e, W)\n",
    "        print(f\"Iteration {iter + 1}, ELBO: {current_ELBO}\")\n",
    "\n",
    "        # Vérification de la convergence\n",
    "        if iter > 0 and np.abs((current_ELBO - prev_ELBO) / prev_ELBO) < tol:\n",
    "            print(f\"Converged after {iter + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        # Mise à jour Gamma\n",
    "        for d in range(D):\n",
    "            G[d, :] = a + np.sum(phi[d], axis=0)\n",
    "\n",
    "        # Mise à jour Lambda\n",
    "        for v in range(V):\n",
    "            L[:, v] = e + np.sum(np.dot(W[d][:, v].T, phi[d]) for d in range(D))\n",
    "\n",
    "        prev_ELBO = current_ELBO\n",
    "    \n",
    "    return L, G, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88160d",
   "metadata": {},
   "source": [
    "**Q3.** Run the algorithm with $K = 10$, $\\alpha = 0.5$, $\\eta = 0.1$. From the results, compute the MMSE of $\\lambda_{kv}$ and $\\gamma_{dk}$.\n",
    "\n",
    "**Bonus** : Re-run the algorithm several times with different initializations, and keep the solution which returns the highest ELBO.\n",
    "\n",
    "NB : In my implementation, one iteration of the CAVI algorithm takes about 4 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f067fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ELBO: -1126928.1606178735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathc\\AppData\\Local\\Temp\\ipykernel_30696\\3454808762.py:52: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  L[:, v] = e + np.sum(np.dot(W[d][:, v].T, phi[d]) for d in range(D))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, ELBO: -496327.67318333546\n",
      "Iteration 3, ELBO: -494222.5006113121\n",
      "Iteration 4, ELBO: -492829.63892522006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m K_max, a_max, e_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (K, a, e) \u001b[38;5;129;01min\u001b[39;00m init:\n\u001b[1;32m----> 5\u001b[0m     L, G, phi \u001b[38;5;241m=\u001b[39m \u001b[43mCAVI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     elbo \u001b[38;5;241m=\u001b[39m ELBO(L, G, phi, a, e, w)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elbo \u001b[38;5;241m>\u001b[39m elbo_max:\n",
      "Cell \u001b[1;32mIn[44], line 52\u001b[0m, in \u001b[0;36mCAVI\u001b[1;34m(W, K, a, e, seed, max_iters, tol)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Mise à jour Lambda\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(V):\n\u001b[1;32m---> 52\u001b[0m         L[:, v] \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     prev_ELBO \u001b[38;5;241m=\u001b[39m current_ELBO\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L, G, phi\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mathc\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2241\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, _gentype):\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# 2018-02-25, 1.15.0\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling np.sum(generator) is deprecated, and in the future will give a different result. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse np.sum(np.fromiter(generator)) or the python sum builtin instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m-> 2241\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_sum_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2243\u001b[0m         out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[1;32mIn[44], line 52\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Mise à jour Lambda\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(V):\n\u001b[1;32m---> 52\u001b[0m         L[:, v] \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(D))\n\u001b[0;32m     54\u001b[0m     prev_ELBO \u001b[38;5;241m=\u001b[39m current_ELBO\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L, G, phi\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = [(10, 0.5, 0.1)]\n",
    "elbo_max = 0\n",
    "K_max, a_max, e_max = 0, 0, 0\n",
    "for (K, a, e) in init:\n",
    "    L, G, phi = CAVI(w, K, a, e, 42)\n",
    "    elbo = ELBO(L, G, phi, a, e, w)\n",
    "    if elbo > elbo_max:\n",
    "        elbo_max = elbo\n",
    "        K_max, a_max, e_max = K, a, e\n",
    "print(f\"Les paramètres d'initialisation pour lesquelles la ELBO est maximale sont K = {K_max}, alpha = {a_max}, eta = {e_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc6553",
   "metadata": {},
   "source": [
    "**Q4.** Based on the MMSE estimates :\n",
    "* What are the top-10 words per topic ? With your machine learning knowledge, can you make sense of some of the topics ?\n",
    "* Choose one document at random and display its topic proportions. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "### YOUR CODE HERE\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f419",
   "metadata": {},
   "source": [
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13019da",
   "metadata": {},
   "source": [
    "**Q5.** Open questions :\n",
    "* What are some limitations of the LDA model ? Can you imagine an improvement ?\n",
    "* In this notebook, we have treated the hyperparameters as fixed. How could they be learned ?\n",
    "* Can you imagine a method to choose the number of topics ?\n",
    "* What strategies should we use to make the algorithm more efficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e6149",
   "metadata": {},
   "source": [
    "**BONUS.** Papier-crayon. À partir du modèle, pouvez-vous dériver les lois conditionnelles de l'échantillonneur de Gibbs ? Pour rappel, nous avons besoin de ces lois pour dériver ensuite les updates de l'algorithme CAVI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
