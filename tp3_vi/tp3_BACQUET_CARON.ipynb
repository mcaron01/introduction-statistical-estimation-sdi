{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5fd37e",
   "metadata": {},
   "source": [
    "# BACQUET Maxime et CARON Mathieu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1296a3d",
   "metadata": {},
   "source": [
    "# TP3 - *Latent Dirichlet Allocation* et Inférence variationnelle \n",
    "\n",
    "## Estimation avancée - G3 SDIA\n",
    "\n",
    "Dans ce TP, on s'intéresse à la méthode \"inférence variationnelle\" (VI) qui permet d'approcher la loi a posteriori d'un modèle (généralement inconnue) par une autre loi plus simple (généralement un produit de lois bien connues). Nous allons l'appliquer à un modèle probabiliste pour des données textuelles, appelé *Latent Dirichlet Allocation* (LDA, qui n'a rien à voir avec la LDA *Linear Discriminant Analysis* du cours de ML).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Renommer votre notebook sous la forme `tp3_Nom1_Nom2.ipynb`, et inclure le nom du binôme dans le notebook. \n",
    "\n",
    "2. Votre code, ainsi que toute sortie du code, doivent être commentés !\n",
    "\n",
    "3. Déposer votre notebook sur Moodle dans la section prévue à cet effet avant la date limite : 23 Décembre 2023, 23h59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d382bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22423210",
   "metadata": {},
   "source": [
    "### Partie 0 - Introduction\n",
    "\n",
    "LDA is a popular probabilistic model for text data, introducted in [Blei et al. (2003)](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). In this model, the posterior distribution is intractable, and we choose to resort to variational inference (note that a Gibbs sampler would be feasible as well, but would be very slow). In particular, the CAVI updates can be easily derived.\n",
    "\n",
    "In a few words, in LDA, each document is a mixture of topics, and each topic is a mixture of words. Uncovering those is the goal of *topic modeling*, and this is what we are going to do today. We will be using a collection of abstracts of papers published in JMLR (*Journal of Machine Learning Research*), one of the most prominent journals of the field.\n",
    "\n",
    "**Check the .pdf file describing the model.**\n",
    "The posterior is :\n",
    "$$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\mathbf{z} | \\mathcal{D}),$$\n",
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059097f",
   "metadata": {},
   "source": [
    "### Partie 1 - Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf0e5b",
   "metadata": {},
   "source": [
    "The data is already prepared, see code below. We have a total of 1898 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41348f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmlr_papers = pkl.load(open(\"jmlr.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694906",
   "metadata": {},
   "source": [
    "**Q1.** Fill in a list of keywords from the course, to see how many papers are about probabilistic ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321a5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 501 Bayesian papers out of 1898\n"
     ]
    }
   ],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = ['bayesian', 'probabilistic', 'Markov', 'mcmc', 'Gibbs sampling', 'posterior', 'prior', \n",
    "                         'likelihood', 'Monte Carlo', 'variational inference']\n",
    "    if any([kwd in paper[\"abstract\"] for kwd in bayesian_keywords]):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "        \n",
    "print(\"There are\", str(len(bayesian_jmlr_papers))+\" Bayesian papers out of\", str(len(jmlr_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a047cf9",
   "metadata": {},
   "source": [
    "Let us now preprocess the data. It is important to remove so-called \"stop-words\" like a, is, but, the, of, have... Scikit-learn will do the job for us. We will keep only the top-1000 words from the abstracts.\n",
    "\n",
    "As a result, we get the count matrix $\\mathbf{C}$ of size $D = 1898 \\times V = 1000$. $c_{dv}$ is the number of occurrences of word $v$ in document $d$. This compact representation is called \"bag-of-words\". Of course from $\\mathbf{C}$ you easily recover the words, since in LDA the order does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855d2359",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform([paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m jmlr_papers])\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m()) \u001b[38;5;66;03m# Top-1000 words\u001b[39;00m\n\u001b[0;32m      6\u001b[0m C \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;66;03m# Count matrix\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Removing documents with 0 words\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, stop_words='english')\n",
    "X = vectorizer.fit_transform([paper[\"abstract\"] for paper in jmlr_papers])\n",
    "print(vectorizer.get_feature_names_out()) # Top-1000 words\n",
    "C = X.toarray() # Count matrix\n",
    "\n",
    "# Removing documents with 0 words\n",
    "idx = np.where(np.sum(C, axis = 1)==0)\n",
    "C = np.delete(C, idx, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e3267",
   "metadata": {},
   "source": [
    "**Q2.** How many elements of $\\mathbf{C}$ are non-zero ? Is this surprising ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2d37bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m number_total_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mC\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(C[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe nombre total d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentrées dans la matrice C est :\u001b[39m\u001b[38;5;124m\"\u001b[39m, number_total_entries)\n\u001b[0;32m      4\u001b[0m num_nonzero_elements \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(C)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "number_total_entries = len(C) * len(C[0])\n",
    "print(\"Le nombre total d'entrées dans la matrice C est :\", number_total_entries)\n",
    "\n",
    "num_nonzero_elements = np.count_nonzero(C)\n",
    "print(\"Le nombre d'éléments non nuls dans la matrice C est :\", num_nonzero_elements)\n",
    "\n",
    "print(f\"La proportion d'éléments non nuls dans cette matrice est donc : {num_nonzero_elements / number_total_entries * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e509bb",
   "metadata": {},
   "source": [
    "Ces résultats montrent que sur les 1895 documents et 1000 mots possibles, seuls 85 804 de ces éléments sont différents de zéro. La représentation du \"bag-of-words\" est donc creuse pour des ensembles de données textuelles (on a une matrice avec beaucoup de 0)\n",
    "\n",
    "Ceci n'est pas surprenant car chaque document utilise généralement seulement un petit sous-ensemble de l'ensemble du vocabulaire, conduisant à une matrice où la majorité des entrées sont nulles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72726d44",
   "metadata": {},
   "source": [
    "### Partie 2 - Inférence variationnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada0b7f",
   "metadata": {},
   "source": [
    "As you know from the lecture, VI aims at maximizing the ELBO. I have prepared for you the function to compute the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cb1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def ELBO(L, G, phi, a, e, W):\n",
    "    # Computes the ELBO with the values of the parameters L (Lambda), G (Gamma), and Phi\n",
    "    # a, e are hyperparameters (alpha and eta)\n",
    "    # W are the words (obsereved)\n",
    "    \n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    # a - Scalar > 0 (hyperparameter alpha)\n",
    "    # e - Scalar > 0 (hyperparameter eta)\n",
    "    # W - List of D elements, each element is a Nd x V matrix (observed words)\n",
    "    (K, V) = L.shape\n",
    "    D = len(phi)\n",
    "    e_log_B = (digamma(L).T - digamma(np.sum(L, axis = 1))).T\n",
    "    e_log_T = (digamma(G).T - digamma(np.sum(G, axis = 1))).T\n",
    "    \n",
    "    t1 = (e-1)*np.sum(e_log_B)\n",
    "    t2 = (a-1)*np.sum(e_log_T)\n",
    "\n",
    "    phi_s = np.zeros((D,K))\n",
    "    for d in range(0,D):\n",
    "        phi_s[d,:] = np.sum(phi[d], axis = 0)\n",
    "    t3 = np.sum(e_log_T*phi_s)\n",
    "    \n",
    "    tmp = np.zeros((K,V))\n",
    "    for d in range(0,D):\n",
    "        tmp = tmp + np.dot(phi[d].T, W[d])\n",
    "    t4 = np.sum(e_log_B*tmp)\n",
    "    \n",
    "    t5 = np.sum(loggamma(np.sum(L, axis = 1))) - np.sum(loggamma(L)) + np.sum((L-1)*e_log_B)\n",
    "    t6 = np.sum(loggamma(np.sum(G, axis = 1))) - np.sum(loggamma(G)) + np.sum((G-1)*e_log_T)\n",
    "\n",
    "    t7 = 0\n",
    "    for d in range(0,D):\n",
    "        t7 = t7 + np.sum(phi[d]*np.log(phi[d] + np.spacing(1)))\n",
    "\n",
    "    return t1 + t2 + t3 + t4 - t5 - t6 - t7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc0302",
   "metadata": {},
   "source": [
    "**Q1.** Transform the matrix $\\mathbf{C}$ into the observed words $\\mathbf{w}$. $\\mathbf{w}$ should be a list of $D$ elements, each element of the list being a $N_d \\times V$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2629d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, V = C.shape\n",
    "\n",
    "# Liste pour stocker les mots observés\n",
    "w = []\n",
    "\n",
    "for doc in range(D):\n",
    "    # On extrait la ligne des counts\n",
    "    counts_d = C[doc, :]\n",
    "    n_d = sum(counts_d)\n",
    "    \n",
    "    # Create a matrix with counts for each word in the vocabulary\n",
    "    matrix_d = np.zeros((n_d, V))\n",
    "\n",
    "    i = 0\n",
    "    k = 0\n",
    "    for v in range(len(counts_d)):\n",
    "        if counts_d[v] != 0:\n",
    "            for j in range(counts_d[v]):\n",
    "                matrix_d[i, k] = 1\n",
    "                i += 1\n",
    "            k += 1\n",
    "    \n",
    "    w.append(matrix_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0aa3d",
   "metadata": {},
   "source": [
    "**Q2.** Implement the CAVI algorithm. The updates are given at the beginning of the notebook. Monitor the convergence with the values of the ELBO (but start with a fixed number of iterations, like 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b24024",
   "metadata": {},
   "source": [
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19389deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI(W, K, a, e, seed, max_iters=50, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Coordinate Ascent Variational Inference (CAVI) for Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "    Parameters:\n",
    "    - W (list): List of D elements, each element is a Nd x V matrix (observed words).\n",
    "    - K (int): Number of topics.\n",
    "    - a (float): Hyperparameter for Gamma distribution (document-topic distribution).\n",
    "    - e (float): Hyperparameter for Gamma distribution (word-topic distribution).\n",
    "    - seed (int): Seed for random number generation.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - tol (float): Convergence tolerance.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Nombre de documents\n",
    "    D = len(W)\n",
    "    # Taille du vocabulaire\n",
    "    V = W[0].shape[1]\n",
    "\n",
    "    # Initialisation des paramètres variationnels\n",
    "    L = np.random.rand(K, V)\n",
    "    G = np.random.rand(D, K)\n",
    "    phi = [np.zeros((Wd.shape[0], K)) for Wd in W]\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Mise à jour phi\n",
    "        for d in range(D):\n",
    "            for n in range(len(phi[d])):\n",
    "                # Indice où il y a le 1 dans la matrice W[d][n, :]\n",
    "                w_dn = np.where(W[d][n, :] == 1)[0][0]\n",
    "                # Calcul de phi_dn pour tous les thèmes en une seule opération\n",
    "                phi[d][n, :] = np.exp(digamma(G[d, :]) + digamma(L[:, w_dn]) - digamma(np.sum(L, axis=1)))\n",
    "                # Normalisation\n",
    "                phi[d][n, :] /= np.sum(phi[d][n, :])\n",
    "\n",
    "        # Calcul de l'ELBO et vérification de la convergence\n",
    "        current_ELBO = ELBO(L, G, phi, a, e, W)\n",
    "        print(f\"Iteration {iter + 1}, ELBO: {current_ELBO}\")\n",
    "\n",
    "        # Vérification de la convergence\n",
    "        if iter > 0 and np.abs((current_ELBO - prev_ELBO) / prev_ELBO) < tol:\n",
    "            print(f\"Converged after {iter + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        # Mise à jour Gamma\n",
    "        for d in range(D):\n",
    "            G[d, :] = a + np.sum(phi[d], axis=0)\n",
    "\n",
    "        # Mise à jour Lambda\n",
    "        for v in range(V):\n",
    "            L[:, v] = e + np.sum(np.dot(W[d][:, v].T, phi[d]) for d in range(D))\n",
    "\n",
    "        prev_ELBO = current_ELBO\n",
    "    \n",
    "    return L, G, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88160d",
   "metadata": {},
   "source": [
    "**Q3.** Run the algorithm with $K = 10$, $\\alpha = 0.5$, $\\eta = 0.1$. From the results, compute the MMSE of $\\lambda_{kv}$ and $\\gamma_{dk}$.\n",
    "\n",
    "**Bonus** : Re-run the algorithm several times with different initializations, and keep the solution which returns the highest ELBO.\n",
    "\n",
    "NB : In my implementation, one iteration of the CAVI algorithm takes about 4 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21d23062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ELBO: -1126928.1606178735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_11820\\3454808762.py:52: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  L[:, v] = e + np.sum(np.dot(W[d][:, v].T, phi[d]) for d in range(D))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, ELBO: -496327.67318333546\n",
      "Iteration 3, ELBO: -494222.5006113121\n",
      "Iteration 4, ELBO: -492829.6389252198\n",
      "Iteration 5, ELBO: -491640.3938148209\n",
      "Iteration 6, ELBO: -490408.0329103642\n",
      "Iteration 7, ELBO: -488969.75641519914\n",
      "Iteration 8, ELBO: -487240.5776350163\n",
      "Iteration 9, ELBO: -485244.84176680376\n",
      "Iteration 10, ELBO: -483142.07131999114\n",
      "Iteration 11, ELBO: -481144.70761278813\n",
      "Iteration 12, ELBO: -479385.1651698362\n",
      "Iteration 13, ELBO: -477880.122661098\n",
      "Iteration 14, ELBO: -476601.225351761\n",
      "Iteration 15, ELBO: -475521.43694675574\n",
      "Iteration 16, ELBO: -474621.3897803674\n",
      "Iteration 17, ELBO: -473876.16825919354\n",
      "Iteration 18, ELBO: -473262.95783701085\n",
      "Iteration 19, ELBO: -472753.0917168412\n",
      "Iteration 20, ELBO: -472315.0009172772\n",
      "Iteration 21, ELBO: -471928.2366804841\n",
      "Iteration 22, ELBO: -471576.3462595348\n",
      "Iteration 23, ELBO: -471250.16884724435\n",
      "Iteration 24, ELBO: -470945.9563372721\n",
      "Iteration 25, ELBO: -470660.42326672934\n",
      "Iteration 26, ELBO: -470391.5960322425\n",
      "Iteration 27, ELBO: -470141.61348033405\n",
      "Iteration 28, ELBO: -469909.5778382041\n",
      "Iteration 29, ELBO: -469692.50970212766\n",
      "Iteration 30, ELBO: -469489.55002488196\n",
      "Iteration 31, ELBO: -469300.3244311405\n",
      "Iteration 32, ELBO: -469126.5145101661\n",
      "Iteration 33, ELBO: -468969.58047881647\n",
      "Iteration 34, ELBO: -468829.2158724654\n",
      "Iteration 35, ELBO: -468705.084633618\n",
      "Iteration 36, ELBO: -468595.22059492616\n",
      "Iteration 37, ELBO: -468497.0609944896\n",
      "Iteration 38, ELBO: -468408.1420551075\n",
      "Iteration 39, ELBO: -468327.1440501933\n",
      "Iteration 40, ELBO: -468252.9172080783\n",
      "Iteration 41, ELBO: -468185.169243071\n",
      "Iteration 42, ELBO: -468123.33878640895\n",
      "Iteration 43, ELBO: -468067.07450223295\n",
      "Iteration 44, ELBO: -468016.18245003244\n",
      "Iteration 45, ELBO: -467969.58985379944\n",
      "Iteration 46, ELBO: -467926.4412788692\n",
      "Iteration 47, ELBO: -467885.7955266096\n",
      "Iteration 48, ELBO: -467847.4169192061\n",
      "Iteration 49, ELBO: -467810.873678085\n",
      "Iteration 50, ELBO: -467776.21074840723\n"
     ]
    }
   ],
   "source": [
    "L, G, phi = CAVI(w, 10, 0.5, 0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6732646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029 9064\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m G_MMSE \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(G)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(L_MMSE, G_MMSE)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "L_MMSE = np.argmax(L)\n",
    "G_MMSE = np.argmax(G)\n",
    "print(L_MMSE, G_MMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f067fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, ELBO: -1126928.1606178735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathc\\AppData\\Local\\Temp\\ipykernel_30696\\3454808762.py:52: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  L[:, v] = e + np.sum(np.dot(W[d][:, v].T, phi[d]) for d in range(D))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, ELBO: -496327.67318333546\n",
      "Iteration 3, ELBO: -494222.5006113121\n",
      "Iteration 4, ELBO: -492829.63892522006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m K_max, a_max, e_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (K, a, e) \u001b[38;5;129;01min\u001b[39;00m init:\n\u001b[1;32m----> 5\u001b[0m     L, G, phi \u001b[38;5;241m=\u001b[39m \u001b[43mCAVI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     elbo \u001b[38;5;241m=\u001b[39m ELBO(L, G, phi, a, e, w)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elbo \u001b[38;5;241m>\u001b[39m elbo_max:\n",
      "Cell \u001b[1;32mIn[44], line 52\u001b[0m, in \u001b[0;36mCAVI\u001b[1;34m(W, K, a, e, seed, max_iters, tol)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Mise à jour Lambda\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(V):\n\u001b[1;32m---> 52\u001b[0m         L[:, v] \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     prev_ELBO \u001b[38;5;241m=\u001b[39m current_ELBO\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L, G, phi\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mathc\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2241\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, _gentype):\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# 2018-02-25, 1.15.0\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling np.sum(generator) is deprecated, and in the future will give a different result. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse np.sum(np.fromiter(generator)) or the python sum builtin instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m-> 2241\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_sum_\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2243\u001b[0m         out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[1;32mIn[44], line 52\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Mise à jour Lambda\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(V):\n\u001b[1;32m---> 52\u001b[0m         L[:, v] \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(D))\n\u001b[0;32m     54\u001b[0m     prev_ELBO \u001b[38;5;241m=\u001b[39m current_ELBO\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L, G, phi\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''init = [(10, 0.5, 0.1)]\n",
    "elbo_max = 0\n",
    "K_max, a_max, e_max = 0, 0, 0\n",
    "for (K, a, e) in init:\n",
    "    L, G, phi = CAVI(w, K, a, e, 42)\n",
    "    elbo = ELBO(L, G, phi, a, e, w)\n",
    "    if elbo > elbo_max:\n",
    "        elbo_max = elbo\n",
    "        K_max, a_max, e_max = K, a, e\n",
    "print(f\"Les paramètres d'initialisation pour lesquelles la ELBO est maximale sont K = {K_max}, alpha = {a_max}, eta = {e_max}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc6553",
   "metadata": {},
   "source": [
    "**Q4.** Based on the MMSE estimates :\n",
    "* What are the top-10 words per topic ? With your machine learning knowledge, can you make sense of some of the topics ?\n",
    "* Choose one document at random and display its topic proportions. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "### YOUR CODE HERE\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f419",
   "metadata": {},
   "source": [
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13019da",
   "metadata": {},
   "source": [
    "**Q5.** Open questions :\n",
    "* What are some limitations of the LDA model ? Can you imagine an improvement ?\n",
    "* In this notebook, we have treated the hyperparameters as fixed. How could they be learned ?\n",
    "* Can you imagine a method to choose the number of topics ?\n",
    "* What strategies should we use to make the algorithm more efficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc4fe9",
   "metadata": {},
   "source": [
    "# Limitations du modèle LDA et Améliorations Potentielles :\n",
    "\n",
    "## Limitations :\n",
    "\n",
    "1. **Supposition du Nombre Fixe de Thèmes :** LDA suppose un nombre fixe de thèmes dans tout le corpus, ce qui peut ne pas refléter avec précision la structure sous-jacente de différentes parties du corpus.\n",
    "\n",
    "2. **Supposition de Mélanges de Thèmes Fixes :** LDA suppose que les mélanges de thèmes sont constants au sein d'un document. Cependant, en réalité, la distribution des thèmes au sein d'un document peut varier.\n",
    "\n",
    "3. **Représentation de Sacs de Mots :** LDA utilise une représentation de sacs de mots, ignorant l'ordre des mots dans les documents, ce qui peut entraîner une perte d'informations sémantiques importantes.\n",
    "\n",
    "4. **Difficulté à Gérer la Polysémie et la Synonymie :** LDA a du mal avec les mots ayant plusieurs significations (polysémie) ou différents mots exprimant des significations similaires (synonymie).\n",
    "\n",
    "## Améliorations Potentielles :\n",
    "\n",
    "1. **Modèles de Thèmes Dynamiques :** Intégrer le temps en tant que facteur peut améliorer LDA pour des applications où les thèmes évoluent au fil du temps.\n",
    "\n",
    "2. **Incorporation de Mots :** Intégrer des incorporations de mots pour capturer les relations sémantiques entre les mots et leurs contextes peut améliorer la compréhension des significations des mots par le modèle.\n",
    "\n",
    "3. **Modèles de Thèmes Corrélés :** Assouplir l'hypothèse d'indépendance des thèmes pour permettre des corrélations entre les thèmes peut conduire à des modèles plus réalistes.\n",
    "\n",
    "# Apprentissage des Hyperparamètres :\n",
    "\n",
    "Les hyperparamètres du modèle LDA, tels que les priors de Dirichlet (α et β), peuvent être appris à partir des données. Quelques approches comprennent :\n",
    "\n",
    "1. **Bayes Empirique :** Estimer les hyperparamètres en fonction des données en maximisant la vraisemblance marginale.\n",
    "\n",
    "2. **Validation Croisée :** Utiliser des techniques de validation croisée pour trouver les valeurs des hyperparamètres qui conduisent à une meilleure généralisation du modèle.\n",
    "\n",
    "# Choix du Nombre de Thèmes :\n",
    "\n",
    "Choisir le bon nombre de thèmes est un aspect crucial de LDA. Plusieurs méthodes peuvent être utilisées :\n",
    "\n",
    "1. **Perplexité :** Utiliser la perplexité sur un ensemble de données détenu pour évaluer le modèle avec différents nombres de thèmes et choisir celui qui minimise la perplexité.\n",
    "\n",
    "2. **Cohérence des Thèmes :** Évaluer la cohérence des thèmes à l'aide de métriques comme C_v ou la cohérence UMass pour trouver un nombre optimal de thèmes.\n",
    "\n",
    "3. **Modèles Hiérarchiques :** Explorer des modèles hiérarchiques de thèmes qui déterminent automatiquement le nombre de thèmes à différents niveaux.\n",
    "\n",
    "# Rendre l'Algorithme Plus Efficace :\n",
    "\n",
    "1. **Inférence Variationnelle en Mini-Batch :** Implémenter une descente de gradient stochastique en mini-batch pour accélérer le processus d'optimisation, particulièrement sur de grands ensembles de données.\n",
    "\n",
    "2. **Parallélisation :** Utiliser le traitement parallèle pour distribuer les calculs sur plusieurs cœurs ou nœuds, en particulier lors des mises à jour de l'étape E.\n",
    "\n",
    "3. **Bibliothèques Optimisées :** Utiliser des bibliothèques ou des frameworks optimisés, tels que des implémentations parallélisées dans des bibliothèques populaires comme scikit-learn ou des frameworks de calcul distribué comme Spark.\n",
    "\n",
    "4. **Inférence Approximative :** Explorer des méthodes d'inférence plus évolutives, telles que l'échantillonnage de Gibbs effondré ou l'inférence variationnelle en ligne.\n",
    "\n",
    "5. **Élagage du Modèle :** Supprimer les mots peu fréquents ou non informatifs pour réduire la taille du vocabulaire et accélérer les calculs.\n",
    "\n",
    "6. **Surveillance de la Convergence :** Mettre en œuvre des mécanismes de surveillance de la convergence pour arrêter le processus d'optimisation lorsque le modèle a convergé, évitant des itérations inutiles.\n",
    "\n",
    "7. **Représentations Sparser :** Exploiter la sparsité des matrices document-thème et thème-mot pour optimiser le stockage et les calculs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e6149",
   "metadata": {},
   "source": [
    "**BONUS.** Papier-crayon. À partir du modèle, pouvez-vous dériver les lois conditionnelles de l'échantillonneur de Gibbs ? Pour rappel, nous avons besoin de ces lois pour dériver ensuite les updates de l'algorithme CAVI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
